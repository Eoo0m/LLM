
<img width="725" alt="preview" src="https://github.com/user-attachments/assets/d2337f62-a818-47a7-8507-8b1acdfb942b" />

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ABCDEFGHIJKL1234567890?usp=sharing)

# 난독화된 한글 리뷰 복원 AI
````markdown



---

## 프로젝트 개요

이 프로젝트는 난독화된 한글 리뷰 문장을 원래의 명확한 형태로 복원하는 인공지능 모델을 개발하는 것을 목표로 합니다.  
예를 들어, 다음과 같은 입력 문장이 있을 때:

> **입력:**  
> 별 한 게토 았깝땀. 왜 싸람듯릭 펼 1캐를 쥰눈징 컥꺾폰 싸람믐롯섞 맒록 섧멍핥쟈  
>  
> **출력:**  
> 별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자

---

##  문제 분석 및 접근 방식

### 기존 T5 모델의 한계

일반 T5 모델을 사용해 학습을 시도했지만, 난독화된 한글은 대부분 `<unk>`(미지의 토큰)으로 처리되어 학습이 되지 않는 문제가 발생했습니다.


input_ids = [3, 2, 3, 2, ..., 1]
tokens     = ['▁', '<unk>', '▁', '<unk>', ..., '</s>']
````

---

### 해결: ByT5 모델 도입

ByT5는 입력을 UTF-8 바이트 단위로 처리하기 때문에 난독화된 텍스트도 잘게 분해하여 처리할 수 있습니다. 다음과 같이 안정적인 토큰 분해가 가능했습니다.

```python
input_ids = [238, 182, 135, 35, 240, ..., 1]
tokens     = ['ë', '³', '\x84', ' ', 'í', ..., '</s>']
```

---

## ⚙️ 학습 설정

다양한 실험을 거쳐 다음과 같은 하이퍼파라미터를 사용했습니다:

* `learning_rate`: **0.00025**
* `weight_decay`: **0.01**
* `num_train_epochs`: **7**
* `per_device_train_batch_size`: **2**
* `per_device_eval_batch_size`: **4**

---

## 실제 복원 예시

> **입력:**
> 녀뮨넒뭅 만죡숭러윤 효템뤼에오. 푸싸눼 옰면 콕 츄쩐학꼬 싶은 콧쉰웨오. 췌꾜윕뉘댜! ㅎㅎ 당음웨 또 옭 컷 갗았요.

> **출력:**
> 너무너무 만족스러운 호텔이에요. 부산에 오면 꼭 추천하고 싶은 곳이에요. 최고입니다! ㅎㅎ 다음에 또 올 것 같아요.

---

> **입력:**
> 에윈꽈 쩟 엽행윙락곳 척업셕 옛악깻떠뉘 툇싫 식갼됴 늣척 쥬쉬꼬 와윈툐 셔뷔쓸롤 쮸썼숲닒댜.
> 쥑원푼뜰 젼붙 췬철핫씩곬 상낭핫셧씁뉘댜. 빵통 꺌끎학곡 붙됴 좋쑵닐댜.
> 빵음잉 살착 않 된는 계 얏쒸었쥠많, 짤 쉬였숩뉜댜. 갊샤해욕!

> **출력:**
> 에이과 첫 여행이라고 적어서 예약했더니 퇴실 시간도 늦춰 주시고 와인도 서비스로 주셨습니다.
> 직원분들 전부 친절하시고 상냥하셨습니다. 방도 깔끔하고 뷰도 좋습니다.
> 방음이 살짝 안 되는 게 아쉬웠지만, 잘 쉬었습니다. 감사해요!


---
