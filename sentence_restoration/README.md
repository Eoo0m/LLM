<img width="725" alt="preview" src="https://github.com/user-attachments/assets/d2337f62-a818-47a7-8507-8b1acdfb942b" />

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/11XIJrvN6FBNS1Ez0JDo1NuOFfN68N3V8?usp=sharing)

````markdown
# 난독화된 한글 리뷰 복원 AI



이 프로젝트는 난독화된 한글 리뷰 문장을 자연스러운 형태로 복원하는 인공지능 모델을 개발하는 것을 목표로 한다.  
예를 들어, 다음과 같은 입력 문장이 있을 때:

> **입력:** '별 한 게토 았깝땀. 왜 싸람듯릭 펼 1캐를 쥰눈징 컥꺾폰 싸람믐롯섞 맒록 섧멍핥쟈'  
> **출력:** '별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자'

처럼, 사람의 의도가 반영된 문장으로 복원하는 것을 목표로 한다.

처음에는 일반적인 T5 모델을 이용해 학습을 시도했지만, 난독화된 한글은 대부분 토크나이저에서 `<unk>`(알 수 없는 토큰)으로 처리되어 학습에 큰 어려움이 있었다.
예를 들어 다음과 같이 입력 텍스트가 거의 전부 `<unk>`로 토큰화되는 문제가 발생했다.

```python
input_ids = [3, 2, 3, 2, ..., 1]
tokens = ['▁', '<unk>', '▁', '<unk>', ..., '</s>']
````

이러한 문제를 해결하기 위해, 글자 단위보다 더 세밀하게 문자를 분해할 수 있는 **ByT5** 모델을 도입했다. ByT5는 입력 문장을 UTF-8 바이트 단위로 분해하여 처리하기 때문에, 기존 T5가 다루기 어려웠던 형태의 텍스트도 효과적으로 표현할 수 있다. 실제로 난독화된 입력에 대해서도 아래와 같이 안정적으로 토큰화가 이루어진다.

```python
input_ids = [238, 182, 135, 35, 240, ..., 1]
tokens = ['ë', '³', '\x84', ' ', 'í', ..., '</s>']
```

모델 학습에는 여러 하이퍼파라미터 실험을 거쳤으며, 다음과 같은 설정이 가장 안정적인 성능을 보였다.

* learning\_rate: **0.00025**
* weight\_decay: **0.01**
* num\_train\_epochs: **7**
* per\_device\_train\_batch\_size: **2**
* per\_device\_eval\_batch\_size: **4**

이러한 설정은 다양한 시도 끝에 empirical하게 결정된 값이며 학습이후 test셋을 시험해본 결과는 다음과 같다.



녀뮨넒뭅 만죡숭러윤 효템뤼에오. 푸싸눼 옰면 콕 츄쩐학꼬 싶은 콧쉰웨오. 췌꾜윕뉘댜! ㅎㅎ 당음웨 또 옭 컷 갗았요.
너무너무 만족스러운 호텔이에요. 부산에 오면 꼭 추천하고 싶은 곳이에요. 최고입니다! ㅎㅎ 다음에 또 올 것 같아요.

에윈꽈 쩟 엽행윙락곳 척업셕 옛악깻떠뉘 툇싫 식갼됴 늣척 쥬쉬꼬 와윈툐 셔뷔쓸롤 쮸썼숲닒댜. 쥑원푼뜰 젼붙 췬철핫씩곬 상낭핫셧씁뉘댜. 빵통 꺌끎학곡 붙됴 좋쑵닐댜. 빵음잉 살착 않 된는 계 얏쒸었쥠많, 짤 쉬였숩뉜댜. 갊샤해욕!
에이과 첫 여행이라고 적어서 예약했더니 퇴실 시간도 늦어 주시고 와인도 서비스로 주셨습니다. 직원분들 전부 친절하시고 사나하셨습니다. 방도 깔끔하고 뷰도 좋습니다. 방음이 살짝 안 되는 게 아쉬웠지만, 잘 쉬었습니다. 감사해요!
